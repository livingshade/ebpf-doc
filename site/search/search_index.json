{"config":{"indexing":"full","lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"Welcome to eBPF docs This is the document about how to implement basic eBPF utility for Rust-based OSs. We focus on how to seperate eBPF utility from kernel-dependent design. Backgrounds We assume you have basic understanding about operating systems and eBPF. The following links may be helpful: Commands mkdocs new [dir-name] - Create a new project. mkdocs serve - Start the live-reloading docs server. mkdocs build - Build the documentation site. mkdocs -h - Print help message and exit. Project layout mkdocs.yml # The configuration file. docs/ index.md # The documentation homepage. ... # Other markdown pages, images and other files.","title":"Home"},{"location":"#welcome-to-ebpf-docs","text":"This is the document about how to implement basic eBPF utility for Rust-based OSs. We focus on how to seperate eBPF utility from kernel-dependent design.","title":"Welcome to eBPF docs"},{"location":"#backgrounds","text":"We assume you have basic understanding about operating systems and eBPF. The following links may be helpful:","title":"Backgrounds"},{"location":"#commands","text":"mkdocs new [dir-name] - Create a new project. mkdocs serve - Start the live-reloading docs server. mkdocs build - Build the documentation site. mkdocs -h - Print help message and exit.","title":"Commands"},{"location":"#project-layout","text":"mkdocs.yml # The configuration file. docs/ index.md # The documentation homepage. ... # Other markdown pages, images and other files.","title":"Project layout"},{"location":"ebpf/","text":"eBPF strcuture eBPF is bascially a method for user to run a program in kernel space. User needs to choose a hookpoint(some kernel functions) and the bpf program. Program should consits only limited eBPF instructions, and the program will be executed when the hookpoint is triggered. To achieve that, kernel needs to provide a way to load program into kernel, a bridge to transfer data between user space and kernel space(BPF maps). In Linux, a dedicated BPF syscall is provided. We will use the same API as Linux, but the implementation is different and much simple. Hook In Linux kernel, eBPF is widely applied in network stack.(i.e. xDP) However, given that some Rust-based OS does not implment netowrk stack, we use kprobes as hookpoint. Since eBPF does not care about the hookpoints, it only needs a instruction to call the BPF program, and, provide necessary contexts. One could imagine that the hookpoint is something like call $bpf_program(arg0 = context) . And for kprobes, the context is the registers. Thus, the implmentation detail of kprobes is not the focus of this project, so we will not discuss it here. One should refer to this for more information. Overview As discussed above, eBPF is a method for user to run a program in kernel space. The program is loaded by a syscall, and the program will be executed when the hookpoint is triggered. The program is a set of instructions, and the program can access data in kernel space through BPF maps. So, for any OS, we need the following components: A new bpf syscall Kernel memory to store the BPF programs and BPF maps The actual implementation of abstract functionalities We will talk about the last component, since the first two are OS-dependent. BPF instructions and helper functions BPF codes are executed in bytecode, and an ISA is provided here . The instruction set is very limited, and it is designed to be safe. Since the BPF instruction are limited, the kernel needs to provide some special helper functions for BPF program to use. For example, bpf_helper_printk is a helper function that prints a string to kernel log. Thus user can read the output to know what is happening. But writing assembly code is hard. So we write C code and use clang to compile it to BPF bytecode. And the helper functions are just symbols in user codes, kernel needs to relocate them to the actual address. BPF maps A BPF map a key-value store in kernel space. The key and value are both fixed size, and the size is defined when the map is created. The map is created by the syscall, and the map is identified by a file descriptor. Note that the communication between BPF program and maps are through file descriptors and helper functions like bpf_helper_map_lookup_elem . There is only two kinds of maps Array and HashTable and their per-CPU version. The difference between Array and HashTable is that Array is indexed by a number, and HashTable is indexed by a key. We omit the per-CPU version and use mutex for concurrency. The maps are no different from their user space corrspodent, except that the map is in kernel space. One could refer to the source code level documentation for more information. load BPF programs A BPF program some thing that user wants to execuate in kernel space. The user program will send the bytecode to kernel space, and the kernel will first verify the bytecode for safty concern. There are also some relocation needs to be. And then the code is JITed into machine code and stored in kernel space. So, it looks like: void *bpf_prog_load(void *prog, size_t prog_len) { if (!verify_and_relocate(prog, prog_len)) { return NULL; } void *machine_code = jit(prog, prog_len); return machine_code; } And when the program needs to be executed, the machine_code is cast to a function pointer and called. We won't talk about verification and JIT here. We simple take them as library. One could refer to this for verification and this for JIT. BPF relocations We can skip verification, but we can't skip relocation. The relocation is needed mainly because the program is loaded into kernel space, and the program needs to access the BPF maps.BPF helper functions also need to be relocated. Note that in Linux kernel, the relocation and verification are coupled and it is more than maps and helpers that needs to be relocated. For simplicity, we will not discuss it here, but one could refer to this for more information.","title":"eBPF overview"},{"location":"ebpf/#ebpf-strcuture","text":"eBPF is bascially a method for user to run a program in kernel space. User needs to choose a hookpoint(some kernel functions) and the bpf program. Program should consits only limited eBPF instructions, and the program will be executed when the hookpoint is triggered. To achieve that, kernel needs to provide a way to load program into kernel, a bridge to transfer data between user space and kernel space(BPF maps). In Linux, a dedicated BPF syscall is provided. We will use the same API as Linux, but the implementation is different and much simple.","title":"eBPF strcuture"},{"location":"ebpf/#hook","text":"In Linux kernel, eBPF is widely applied in network stack.(i.e. xDP) However, given that some Rust-based OS does not implment netowrk stack, we use kprobes as hookpoint. Since eBPF does not care about the hookpoints, it only needs a instruction to call the BPF program, and, provide necessary contexts. One could imagine that the hookpoint is something like call $bpf_program(arg0 = context) . And for kprobes, the context is the registers. Thus, the implmentation detail of kprobes is not the focus of this project, so we will not discuss it here. One should refer to this for more information.","title":"Hook"},{"location":"ebpf/#overview","text":"As discussed above, eBPF is a method for user to run a program in kernel space. The program is loaded by a syscall, and the program will be executed when the hookpoint is triggered. The program is a set of instructions, and the program can access data in kernel space through BPF maps. So, for any OS, we need the following components: A new bpf syscall Kernel memory to store the BPF programs and BPF maps The actual implementation of abstract functionalities We will talk about the last component, since the first two are OS-dependent.","title":"Overview"},{"location":"ebpf/#bpf-instructions-and-helper-functions","text":"BPF codes are executed in bytecode, and an ISA is provided here . The instruction set is very limited, and it is designed to be safe. Since the BPF instruction are limited, the kernel needs to provide some special helper functions for BPF program to use. For example, bpf_helper_printk is a helper function that prints a string to kernel log. Thus user can read the output to know what is happening. But writing assembly code is hard. So we write C code and use clang to compile it to BPF bytecode. And the helper functions are just symbols in user codes, kernel needs to relocate them to the actual address.","title":"BPF instructions and helper functions"},{"location":"ebpf/#bpf-maps","text":"A BPF map a key-value store in kernel space. The key and value are both fixed size, and the size is defined when the map is created. The map is created by the syscall, and the map is identified by a file descriptor. Note that the communication between BPF program and maps are through file descriptors and helper functions like bpf_helper_map_lookup_elem . There is only two kinds of maps Array and HashTable and their per-CPU version. The difference between Array and HashTable is that Array is indexed by a number, and HashTable is indexed by a key. We omit the per-CPU version and use mutex for concurrency. The maps are no different from their user space corrspodent, except that the map is in kernel space. One could refer to the source code level documentation for more information.","title":"BPF maps"},{"location":"ebpf/#load-bpf-programs","text":"A BPF program some thing that user wants to execuate in kernel space. The user program will send the bytecode to kernel space, and the kernel will first verify the bytecode for safty concern. There are also some relocation needs to be. And then the code is JITed into machine code and stored in kernel space. So, it looks like: void *bpf_prog_load(void *prog, size_t prog_len) { if (!verify_and_relocate(prog, prog_len)) { return NULL; } void *machine_code = jit(prog, prog_len); return machine_code; } And when the program needs to be executed, the machine_code is cast to a function pointer and called. We won't talk about verification and JIT here. We simple take them as library. One could refer to this for verification and this for JIT.","title":"load BPF programs"},{"location":"ebpf/#bpf-relocations","text":"We can skip verification, but we can't skip relocation. The relocation is needed mainly because the program is loaded into kernel space, and the program needs to access the BPF maps.BPF helper functions also need to be relocated. Note that in Linux kernel, the relocation and verification are coupled and it is more than maps and helpers that needs to be relocated. For simplicity, we will not discuss it here, but one could refer to this for more information.","title":"BPF relocations"},{"location":"rcore/","text":"migrate eBPF to rCore tutorial This is a step-by-step tutorial of how to migrate eBPF to rCore. Environment Setup Note that rCore tutorial only provides rust user programs but their is no rust support for eBPF yet. In that case, we need to use uCore-test to compile the user programs because uCore and rCore share the same ABI. First, clone the repo git clone https://github.com/cubele/rCore-Tutorial-Code-2022A rcore-ebpf cd rcore-ebpf Install dependencies We need rust, llvm, clang, qemu and musl toolchains. Change config You need to mannually change the path config in user/ebpf/build.sh , you basically just need to change the prefix of builddir, ucoredir, targetdir , use absolute path. cd os8 make run You should see that rCore is started and the application is running. Run ebpf_user_loadprogextest and ebpf_user_kernmaptest to see the effect. Migration According to the [eBPF tutorial], we now needs to add some os-dependent code to make it work. We only needs to add the syscall interface and implement the osutils.rs to provide necessary kernel functions. We should always bear in mind that rCore kernel has its own page table, rather than those share with user space, like Linux and zCore. Add syscall interface We need to add the following syscall interface to syscall.rs : First, add the syscall number to syscall/mod.rs : //! syscall/mod.rs const SYSCALL_BPF: usize = 280; // ... pub fn syscall(syscall_id: usize, args: [usize; 4]) -> isize { match syscall_id { // ... SYSCALL_BPF => sys_bpf(args[0] as isize, args[1] as usize, args[2] as usize), } Then, implement sys_bpf //! syscall/ebpf.rs pub fn sys_bpf(cmd: isize, bpf_attr: usize , size: usize) -> isize { let ptr = bpf_attr as *const u8; let cmd = cmd as i32; use crate::ebpf::BpfCommand::*; if let Ok(bpf_cmd) = BpfCommand::try_from(cmd) { let ret = match bpf_cmd { BPF_MAP_CREATE => sys_bpf_map_create(ptr, size), BPF_MAP_LOOKUP_ELEM => sys_bpf_map_lookup_elem(ptr, size), BPF_MAP_UPDATE_ELEM => sys_bpf_map_update_elem(ptr, size), BPF_MAP_DELETE_ELEM => sys_bpf_map_delete_elem(ptr, size), BPF_MAP_GET_NEXT_KEY => sys_bpf_map_get_next_key(ptr, size), BPF_PROG_LOAD => todo!(), BPF_PROG_ATTACH => sys_bpf_program_attach(ptr, size), BPF_PROG_DETACH => sys_bpf_program_detach(ptr, size), BPF_PROG_LOAD_EX => sys_preprocess_bpf_program_load_ex(ptr, size), }; if ret < 0 { -1 } else { ret as isize } } else { -1 } We just need to add the syscall interface and use a match to call the corresponding function. We pass the attr as argument and let the function to handle it. Note that BpfCommand is an enum that represents the command type following the Linux API. And we are finished with the syscall interface. This is quite simple, isn't it? Implement osutils.rs In osutils.rs , we need to implement the following parts: A trait ThreadLike that is an analog to Linux thread. It should have get_pid , get_tid and get_name methods. And the function os_current_thread that returns that thread-like object. In rCore, we know that TaskControlBlock is the thread-like object. So we can implement the trait for TaskControlBlock and return the current thread. impl ThreadLike for TaskControlBlock { fn get_pid(&self) -> u64 { let proc = self.process.upgrade().unwrap(); return proc.pid.0 as u64; } fn get_tid(&self) -> u64 { return 0; // no viable in rcore tutor } fn get_name(&self) -> String { return String::from(\"not viable in rcore tutorial\") } } Note that we don't have a tid in rCore. So we just return 0. And we don't have a process name. So we just return a dummy string. Also, we need to implement the os_current_thread function. pub fn os_current_thread() -> Arc<dyn ThreadLike> { if let Some(thread) = crate::task::current_task() { thread } else { panic!(\"cannot get current thread!\") } } os_current_time that returns the current time in nanoseconds. os_get_current_cpu that returns the current cpu id. os_console_write_str that writes a string to the console. Those are used in helper functions. pub fn os_current_time() -> u128 { crate::timer::get_time_us() as u128 * 1000 } pub fn os_get_current_cpu() -> u8 { 0 // not viable } pub fn os_console_write_str(s: &str) { crate::console::Stdout.write_str(s).unwrap(); } Again, rCore a single-core system. So we just return 0 for os_get_current_cpu . And we use Stdout to write to the console. os_copy_from_user and os_copy_to_user that copy data from/to user space. Because some OS use virtual memory, we need to translate the address. This two functions are primitives for other memory related functions like get_generic_from_user and all other syscall branches. pub fn os_copy_from_user(usr_addr: usize, kern_buf: *mut u8, len: usize) -> i32 { use crate::mm::translated_byte_buffer; use crate::task::current_user_token; let t = translated_byte_buffer(current_user_token(), usr_addr as *const u8, len); let mut all = vec![]; for i in t { all.extend(i.to_vec()); } copy(kern_buf, all.as_ptr() as *const u8, len); 0 } pub fn os_copy_to_user(usr_addr: usize, kern_buf: *const u8, len: usize) -> i32 { use crate::mm::translated_byte_buffer; use crate::task::current_user_token; let dst = translated_byte_buffer(current_user_token(), usr_addr as *const u8, len); let mut ptr = kern_buf; let mut total_len = len as i32; for seg in dst { let cur_len = seg.len(); total_len -= cur_len as i32; unsafe { core::ptr::copy_nonoverlapping(ptr, seg.as_mut_ptr(), cur_len); ptr = ptr.add(cur_len); } } assert_eq!(total_len, 0); 0 } // You don't need to change this two functions pub fn copy(dst: *mut u8, src: *const u8, len: usize) { let from = unsafe { from_raw_parts(src, len) }; let to = unsafe { from_raw_parts_mut(dst, len) }; to.copy_from_slice(from); } pub fn memcmp(u: *const u8, v: *const u8, len: usize) -> bool { return unsafe { from_raw_parts(u, len) == from_raw_parts(v, len) } } This part is very tricky, since rCore use different page table for kernel and user space. So for every pointer from user space, we needs to translate and copy it to kernel buffer and vice versa. We use translated_byte_buffer to translate the address, which is provided by mm . It is worth note that for zCore, which use the same page table for kernel and user space, the two function is just a memcpy . And that is all for osutils.rs ! You can refer to the source code to see we use this two primitives to build get_generic_from_user , and use that to get the attr from user space. And all other syscall branches are done. That is because their logic is os-independent, we only needs to provide a memory interface. BPF map operations We don't need to change any code here, but I want to show that how different page table effect the migration. Nte that functions like bpf_map_lookup_elem can be called both from kernel space and user space. It is nothing important in zCore, since zCore use the same page table for kernel and user space. But in rCore, we need to be especially careful. When a user program wants to access a map, it needs to call bpf_map_lookup_elem in user space. And the syscall interface will call sys_bpf_map_lookup_elem in kernel space. In that case, address are from user space, and we need the additional translation. But a loaded BPF program can access maps as well and it runs in kernel space. In that case, the address is from kernel space, and we don't need the translation. So, we need to pass a flag to tell the function whether the address is from user space or not. Argument from_user: bool in that case. For user program, it calls through sys_bpf_map_... , which passes from_user = true . For BPF program, it calls bpf_helper_map_... , which passes from_user = false . Let's take a look at the code of bpf_map_ops , in which the actual map operations are done. pub fn bpf_map_ops(fd: u32, op: BpfMapOp, key: *const u8, value: *mut u8, flags: u64, from_user: bool) -> BpfResult { let bpf_objs = BPF_OBJECTS.lock(); let obj = bpf_objs.get(&fd).ok_or(ENOENT)?; let shared_map = obj.is_map().ok_or(ENOENT)?; let mut map = shared_map.lock(); if from_user { let key_size = map.get_attr().key_size; let value_size = map.get_attr().value_size; let mut key_kern_buf = alloc::vec![0 as u8; key_size]; let kptr = key_kern_buf.as_mut_ptr(); os_copy_from_user(key as usize, kptr, key_size); let mut value_kern_buf = alloc::vec![0 as u8; value_size]; let vptr = value_kern_buf.as_mut_ptr(); match op { BpfMapOp::LookUp => { let ret = map.lookup(kptr, vptr); os_copy_to_user(value as usize, vptr, value_size); ret }, BpfMapOp::Update => { os_copy_from_user(value as usize, vptr, value_size); let ret = map.update(kptr, vptr, flags); ret }, BpfMapOp::Delete => map.delete(kptr), BpfMapOp::GetNextKey => { let ret = map.next_key(kptr, vptr); os_copy_to_user(value as usize, vptr, value_size); ret } _ => Err(EINVAL), } } else { match op { BpfMapOp::LookUp => map.lookup(key, value), BpfMapOp::Update => map.update(key, value, flags), BpfMapOp::Delete => map.delete(key), BpfMapOp::GetNextKey => map.next_key(key, value), _ => Err(EINVAL), } } } We can see that we need extra copy when from_user is true. And we use os_copy_from_user and os_copy_to_user to do the copy. For OS like zCore, those steps are redundant. An extra copy will make it run slower, but does not harm the correctness. This is the fundamental trade off between generality and performance. Add user program Now we need add user program that test our eBPF utility. There are 4 user tests. ebpf_user_naivetest.c is a naive test that just call sys_bpf to test the syscall interface. ebpf_user_maptest.c is a test that test the map operations from user program. ebpf_user_loadprogextest.c is a test that load a BPF kernel program context.c from user program and print out the registers. This tests the kprobe and some helper functions. ebpf_user_kernmaptest.c is a test that load a BPF kernel program map.c from user program and test the map operations from kernel program. This tests the bpf map in kernel space. You can simple run them in make run . Add more user programs If you want add more user programs, you need to add the source file in ucore/src just like the 4 tests above. If you want to add kernel BPF programs and load them, you need to add the kernel source file in user/ebpf/kern just like map.c and context.c , and do the following thing: # suppose you add map2.c as kernal program cd user/ebpf/kern make python3 hex2char.py # copy the output, which is the length of kern prog # copy map2.dump to your user program, take ebpf_user_loadprogextest.c as example That is because rCore tutorial does not support mmap or malloc , we have to mannually copy the byte code into a char [] array. hex2char.py just read a ELF and print out its hex value byte by byte. We also have to mannually set the length of the program as argument. You can refer to zCore test program if you have implmented mmap or malloc and fstat for kernel. Don't forget to change build.sh before you run make run in os8 Details about Makefile You can skip this section if you are not interested in the details. But if you want to migrate from scratch, you need to know the whole procedure. We need to compile the user program and link it with our library. We just use uCore-Test's makefile because its compatitble with rCore. But for eBPF programs, we need a user library bpf.h and bpf.c , in ucore/include , ucore/lib , respectively. When we call make in /ucore , it will compile the user program and the ELF is in ucore/build/riscv64 . Then we need to copy the elf to rCore's filesystem, which is done by user/ebpf/build.sh . In the script, we touch a empty file called [testcase].rs and copy the elf to target directory. Then we run make run in os8 . You can refer to makefile fs-img and fn easy_fs_pack for the reason. Those steps are quiet stupid and you might find better solution. Further work Currently, the symbol table is not supported, thus the kprobe attach target is hardcoded in tracepoint.rs::resolve_symbol as sys_open . You can try to support it by adding the symbol table utility. You can refer to zCore's implementation.","title":"migrate to rCore tutorial"},{"location":"rcore/#migrate-ebpf-to-rcore-tutorial","text":"This is a step-by-step tutorial of how to migrate eBPF to rCore.","title":"migrate eBPF to rCore tutorial"},{"location":"rcore/#environment-setup","text":"Note that rCore tutorial only provides rust user programs but their is no rust support for eBPF yet. In that case, we need to use uCore-test to compile the user programs because uCore and rCore share the same ABI. First, clone the repo git clone https://github.com/cubele/rCore-Tutorial-Code-2022A rcore-ebpf cd rcore-ebpf","title":"Environment Setup"},{"location":"rcore/#install-dependencies","text":"We need rust, llvm, clang, qemu and musl toolchains.","title":"Install dependencies"},{"location":"rcore/#change-config","text":"You need to mannually change the path config in user/ebpf/build.sh , you basically just need to change the prefix of builddir, ucoredir, targetdir , use absolute path. cd os8 make run You should see that rCore is started and the application is running. Run ebpf_user_loadprogextest and ebpf_user_kernmaptest to see the effect.","title":"Change config"},{"location":"rcore/#migration","text":"According to the [eBPF tutorial], we now needs to add some os-dependent code to make it work. We only needs to add the syscall interface and implement the osutils.rs to provide necessary kernel functions. We should always bear in mind that rCore kernel has its own page table, rather than those share with user space, like Linux and zCore.","title":"Migration"},{"location":"rcore/#add-syscall-interface","text":"We need to add the following syscall interface to syscall.rs : First, add the syscall number to syscall/mod.rs : //! syscall/mod.rs const SYSCALL_BPF: usize = 280; // ... pub fn syscall(syscall_id: usize, args: [usize; 4]) -> isize { match syscall_id { // ... SYSCALL_BPF => sys_bpf(args[0] as isize, args[1] as usize, args[2] as usize), } Then, implement sys_bpf //! syscall/ebpf.rs pub fn sys_bpf(cmd: isize, bpf_attr: usize , size: usize) -> isize { let ptr = bpf_attr as *const u8; let cmd = cmd as i32; use crate::ebpf::BpfCommand::*; if let Ok(bpf_cmd) = BpfCommand::try_from(cmd) { let ret = match bpf_cmd { BPF_MAP_CREATE => sys_bpf_map_create(ptr, size), BPF_MAP_LOOKUP_ELEM => sys_bpf_map_lookup_elem(ptr, size), BPF_MAP_UPDATE_ELEM => sys_bpf_map_update_elem(ptr, size), BPF_MAP_DELETE_ELEM => sys_bpf_map_delete_elem(ptr, size), BPF_MAP_GET_NEXT_KEY => sys_bpf_map_get_next_key(ptr, size), BPF_PROG_LOAD => todo!(), BPF_PROG_ATTACH => sys_bpf_program_attach(ptr, size), BPF_PROG_DETACH => sys_bpf_program_detach(ptr, size), BPF_PROG_LOAD_EX => sys_preprocess_bpf_program_load_ex(ptr, size), }; if ret < 0 { -1 } else { ret as isize } } else { -1 } We just need to add the syscall interface and use a match to call the corresponding function. We pass the attr as argument and let the function to handle it. Note that BpfCommand is an enum that represents the command type following the Linux API. And we are finished with the syscall interface. This is quite simple, isn't it?","title":"Add syscall interface"},{"location":"rcore/#implement-osutilsrs","text":"In osutils.rs , we need to implement the following parts: A trait ThreadLike that is an analog to Linux thread. It should have get_pid , get_tid and get_name methods. And the function os_current_thread that returns that thread-like object. In rCore, we know that TaskControlBlock is the thread-like object. So we can implement the trait for TaskControlBlock and return the current thread. impl ThreadLike for TaskControlBlock { fn get_pid(&self) -> u64 { let proc = self.process.upgrade().unwrap(); return proc.pid.0 as u64; } fn get_tid(&self) -> u64 { return 0; // no viable in rcore tutor } fn get_name(&self) -> String { return String::from(\"not viable in rcore tutorial\") } } Note that we don't have a tid in rCore. So we just return 0. And we don't have a process name. So we just return a dummy string. Also, we need to implement the os_current_thread function. pub fn os_current_thread() -> Arc<dyn ThreadLike> { if let Some(thread) = crate::task::current_task() { thread } else { panic!(\"cannot get current thread!\") } } os_current_time that returns the current time in nanoseconds. os_get_current_cpu that returns the current cpu id. os_console_write_str that writes a string to the console. Those are used in helper functions. pub fn os_current_time() -> u128 { crate::timer::get_time_us() as u128 * 1000 } pub fn os_get_current_cpu() -> u8 { 0 // not viable } pub fn os_console_write_str(s: &str) { crate::console::Stdout.write_str(s).unwrap(); } Again, rCore a single-core system. So we just return 0 for os_get_current_cpu . And we use Stdout to write to the console. os_copy_from_user and os_copy_to_user that copy data from/to user space. Because some OS use virtual memory, we need to translate the address. This two functions are primitives for other memory related functions like get_generic_from_user and all other syscall branches. pub fn os_copy_from_user(usr_addr: usize, kern_buf: *mut u8, len: usize) -> i32 { use crate::mm::translated_byte_buffer; use crate::task::current_user_token; let t = translated_byte_buffer(current_user_token(), usr_addr as *const u8, len); let mut all = vec![]; for i in t { all.extend(i.to_vec()); } copy(kern_buf, all.as_ptr() as *const u8, len); 0 } pub fn os_copy_to_user(usr_addr: usize, kern_buf: *const u8, len: usize) -> i32 { use crate::mm::translated_byte_buffer; use crate::task::current_user_token; let dst = translated_byte_buffer(current_user_token(), usr_addr as *const u8, len); let mut ptr = kern_buf; let mut total_len = len as i32; for seg in dst { let cur_len = seg.len(); total_len -= cur_len as i32; unsafe { core::ptr::copy_nonoverlapping(ptr, seg.as_mut_ptr(), cur_len); ptr = ptr.add(cur_len); } } assert_eq!(total_len, 0); 0 } // You don't need to change this two functions pub fn copy(dst: *mut u8, src: *const u8, len: usize) { let from = unsafe { from_raw_parts(src, len) }; let to = unsafe { from_raw_parts_mut(dst, len) }; to.copy_from_slice(from); } pub fn memcmp(u: *const u8, v: *const u8, len: usize) -> bool { return unsafe { from_raw_parts(u, len) == from_raw_parts(v, len) } } This part is very tricky, since rCore use different page table for kernel and user space. So for every pointer from user space, we needs to translate and copy it to kernel buffer and vice versa. We use translated_byte_buffer to translate the address, which is provided by mm . It is worth note that for zCore, which use the same page table for kernel and user space, the two function is just a memcpy . And that is all for osutils.rs ! You can refer to the source code to see we use this two primitives to build get_generic_from_user , and use that to get the attr from user space. And all other syscall branches are done. That is because their logic is os-independent, we only needs to provide a memory interface.","title":"Implement osutils.rs"},{"location":"rcore/#bpf-map-operations","text":"We don't need to change any code here, but I want to show that how different page table effect the migration. Nte that functions like bpf_map_lookup_elem can be called both from kernel space and user space. It is nothing important in zCore, since zCore use the same page table for kernel and user space. But in rCore, we need to be especially careful. When a user program wants to access a map, it needs to call bpf_map_lookup_elem in user space. And the syscall interface will call sys_bpf_map_lookup_elem in kernel space. In that case, address are from user space, and we need the additional translation. But a loaded BPF program can access maps as well and it runs in kernel space. In that case, the address is from kernel space, and we don't need the translation. So, we need to pass a flag to tell the function whether the address is from user space or not. Argument from_user: bool in that case. For user program, it calls through sys_bpf_map_... , which passes from_user = true . For BPF program, it calls bpf_helper_map_... , which passes from_user = false . Let's take a look at the code of bpf_map_ops , in which the actual map operations are done. pub fn bpf_map_ops(fd: u32, op: BpfMapOp, key: *const u8, value: *mut u8, flags: u64, from_user: bool) -> BpfResult { let bpf_objs = BPF_OBJECTS.lock(); let obj = bpf_objs.get(&fd).ok_or(ENOENT)?; let shared_map = obj.is_map().ok_or(ENOENT)?; let mut map = shared_map.lock(); if from_user { let key_size = map.get_attr().key_size; let value_size = map.get_attr().value_size; let mut key_kern_buf = alloc::vec![0 as u8; key_size]; let kptr = key_kern_buf.as_mut_ptr(); os_copy_from_user(key as usize, kptr, key_size); let mut value_kern_buf = alloc::vec![0 as u8; value_size]; let vptr = value_kern_buf.as_mut_ptr(); match op { BpfMapOp::LookUp => { let ret = map.lookup(kptr, vptr); os_copy_to_user(value as usize, vptr, value_size); ret }, BpfMapOp::Update => { os_copy_from_user(value as usize, vptr, value_size); let ret = map.update(kptr, vptr, flags); ret }, BpfMapOp::Delete => map.delete(kptr), BpfMapOp::GetNextKey => { let ret = map.next_key(kptr, vptr); os_copy_to_user(value as usize, vptr, value_size); ret } _ => Err(EINVAL), } } else { match op { BpfMapOp::LookUp => map.lookup(key, value), BpfMapOp::Update => map.update(key, value, flags), BpfMapOp::Delete => map.delete(key), BpfMapOp::GetNextKey => map.next_key(key, value), _ => Err(EINVAL), } } } We can see that we need extra copy when from_user is true. And we use os_copy_from_user and os_copy_to_user to do the copy. For OS like zCore, those steps are redundant. An extra copy will make it run slower, but does not harm the correctness. This is the fundamental trade off between generality and performance.","title":"BPF map operations"},{"location":"rcore/#add-user-program","text":"Now we need add user program that test our eBPF utility. There are 4 user tests. ebpf_user_naivetest.c is a naive test that just call sys_bpf to test the syscall interface. ebpf_user_maptest.c is a test that test the map operations from user program. ebpf_user_loadprogextest.c is a test that load a BPF kernel program context.c from user program and print out the registers. This tests the kprobe and some helper functions. ebpf_user_kernmaptest.c is a test that load a BPF kernel program map.c from user program and test the map operations from kernel program. This tests the bpf map in kernel space. You can simple run them in make run .","title":"Add user program"},{"location":"rcore/#add-more-user-programs","text":"If you want add more user programs, you need to add the source file in ucore/src just like the 4 tests above. If you want to add kernel BPF programs and load them, you need to add the kernel source file in user/ebpf/kern just like map.c and context.c , and do the following thing: # suppose you add map2.c as kernal program cd user/ebpf/kern make python3 hex2char.py # copy the output, which is the length of kern prog # copy map2.dump to your user program, take ebpf_user_loadprogextest.c as example That is because rCore tutorial does not support mmap or malloc , we have to mannually copy the byte code into a char [] array. hex2char.py just read a ELF and print out its hex value byte by byte. We also have to mannually set the length of the program as argument. You can refer to zCore test program if you have implmented mmap or malloc and fstat for kernel. Don't forget to change build.sh before you run make run in os8","title":"Add more user programs"},{"location":"rcore/#details-about-makefile","text":"You can skip this section if you are not interested in the details. But if you want to migrate from scratch, you need to know the whole procedure. We need to compile the user program and link it with our library. We just use uCore-Test's makefile because its compatitble with rCore. But for eBPF programs, we need a user library bpf.h and bpf.c , in ucore/include , ucore/lib , respectively. When we call make in /ucore , it will compile the user program and the ELF is in ucore/build/riscv64 . Then we need to copy the elf to rCore's filesystem, which is done by user/ebpf/build.sh . In the script, we touch a empty file called [testcase].rs and copy the elf to target directory. Then we run make run in os8 . You can refer to makefile fs-img and fn easy_fs_pack for the reason. Those steps are quiet stupid and you might find better solution.","title":"Details about Makefile"},{"location":"rcore/#further-work","text":"Currently, the symbol table is not supported, thus the kprobe attach target is hardcoded in tracepoint.rs::resolve_symbol as sys_open . You can try to support it by adding the symbol table utility. You can refer to zCore's implementation.","title":"Further work"},{"location":"zcore/","text":"implment eBPF utility in zCore This is a source code level documentation for zCore.","title":"An implement in zCore"},{"location":"zcore/#implment-ebpf-utility-in-zcore","text":"This is a source code level documentation for zCore.","title":"implment eBPF utility in zCore"}]}